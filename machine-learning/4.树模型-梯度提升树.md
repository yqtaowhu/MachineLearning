- [树模型-梯度提升树](#%e6%a0%91%e6%a8%a1%e5%9e%8b-%e6%a2%af%e5%ba%a6%e6%8f%90%e5%8d%87%e6%a0%91)
  - [1. Boost算法](#1-boost%e7%ae%97%e6%b3%95)
  - [2. AdaBoost](#2-adaboost)
  - [3. GBDT](#3-gbdt)
  - [4. XGBoost](#4-xgboost)
    - [4.1 xgboost算法原理](#41-xgboost%e7%ae%97%e6%b3%95%e5%8e%9f%e7%90%86)
    - [4.2 XGBoost和传统GBDT的区别](#42-xgboost%e5%92%8c%e4%bc%a0%e7%bb%9fgbdt%e7%9a%84%e5%8c%ba%e5%88%ab)
    - [4.3 xgboost的常见问题](#43-xgboost%e7%9a%84%e5%b8%b8%e8%a7%81%e9%97%ae%e9%a2%98)
      - [4.3.1 如何防止过拟合](#431-%e5%a6%82%e4%bd%95%e9%98%b2%e6%ad%a2%e8%bf%87%e6%8b%9f%e5%90%88)
      - [4.3.2 xgboost为什么要进行二次的展开](#432-xgboost%e4%b8%ba%e4%bb%80%e4%b9%88%e8%a6%81%e8%bf%9b%e8%a1%8c%e4%ba%8c%e6%ac%a1%e7%9a%84%e5%b1%95%e5%bc%80)
      - [4.3.3 xgboost为什么可以并行训练](#433-xgboost%e4%b8%ba%e4%bb%80%e4%b9%88%e5%8f%af%e4%bb%a5%e5%b9%b6%e8%a1%8c%e8%ae%ad%e7%bb%83)
      - [4.3.4 xgboost为什么快](#434-xgboost%e4%b8%ba%e4%bb%80%e4%b9%88%e5%bf%ab)
      - [4.3.5 xgboost如何处理缺失值](#435-xgboost%e5%a6%82%e4%bd%95%e5%a4%84%e7%90%86%e7%bc%ba%e5%a4%b1%e5%80%bc)
      - [4.3.6 RF和GBDT的区别](#436-rf%e5%92%8cgbdt%e7%9a%84%e5%8c%ba%e5%88%ab)
      - [4.3.7 RF和GBDT的区别](#437-rf%e5%92%8cgbdt%e7%9a%84%e5%8c%ba%e5%88%ab)
      - [4.3.8 比较LR和GBDT，说说什么情景下GBDT不如LR](#438-%e6%af%94%e8%be%83lr%e5%92%8cgbdt%e8%af%b4%e8%af%b4%e4%bb%80%e4%b9%88%e6%83%85%e6%99%af%e4%b8%8bgbdt%e4%b8%8d%e5%a6%82lr)
      - [4.3.9 数的复杂度如何衡量](#439-%e6%95%b0%e7%9a%84%e5%a4%8d%e6%9d%82%e5%ba%a6%e5%a6%82%e4%bd%95%e8%a1%a1%e9%87%8f)
      - [4.3.10 xgboost 如何对树进行剪枝](#4310-xgboost-%e5%a6%82%e4%bd%95%e5%af%b9%e6%a0%91%e8%bf%9b%e8%a1%8c%e5%89%aa%e6%9e%9d)
      - [4.3.11 如何选择最优的分裂点](#4311-%e5%a6%82%e4%bd%95%e9%80%89%e6%8b%a9%e6%9c%80%e4%bc%98%e7%9a%84%e5%88%86%e8%a3%82%e7%82%b9)
      - [4.3.12 如何选择最优的分裂点](#4312-%e5%a6%82%e4%bd%95%e9%80%89%e6%8b%a9%e6%9c%80%e4%bc%98%e7%9a%84%e5%88%86%e8%a3%82%e7%82%b9)
      - [4.3.13 如何调参](#4313-%e5%a6%82%e4%bd%95%e8%b0%83%e5%8f%82)
      - [4.3.14 为什么XGBoost相比某些模型对缺失值不敏感](#4314-%e4%b8%ba%e4%bb%80%e4%b9%88xgboost%e7%9b%b8%e6%af%94%e6%9f%90%e4%ba%9b%e6%a8%a1%e5%9e%8b%e5%af%b9%e7%bc%ba%e5%a4%b1%e5%80%bc%e4%b8%8d%e6%95%8f%e6%84%9f)
  - [5. LightGBM](#5-lightgbm)
    - [5.1 lightgbm 和xgboost的区别](#51-lightgbm-%e5%92%8cxgboost%e7%9a%84%e5%8c%ba%e5%88%ab)
  - [参考资料](#%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99)


# 树模型-梯度提升树

## 1. Boost算法

## 2. AdaBoost


## 3. GBDT


## 4. XGBoost

### 4.1 xgboost算法原理

**object:**

$$obj(\theta) = \sum_i^n l(y_i, y\hat{}) + \sum_k^K \Omega(f_t)$$

加性模型：
$$\hat{y}^t = \hat{y}^{t-1} + f_t(x)$$

所以:

$$obj(\theta)^{(t)} = \sum_i^n l[y_i, \hat{y}^{t-1} + f_t(x)] + \Omega(f_t) + constant$$


![分裂查找](https://pic1.zhimg.com/80/d149e74f35bee9f7e8e254b654a3fba0_hd.jpg)


在XGBoost中，出于性能优化的考虑，也提供了近似的建模算法支持，核心思想是在寻找split point的时候，不会枚举所有的特征值，而会对特征值进行聚合统计，然后形成若干个bucket，只将bucket边界上的特征值作为split point的候选，从而获得性能提升

![近似查找分裂](https://pic2.zhimg.com/80/bb73b7e85b8f5f0015f7fda22972618d_hd.jpg)

![稀疏分裂查找](https://pic4.zhimg.com/80/514b097f4eb5ac33bd180fbb6f26bdf4_hd.jpg)

对于稀疏性的离散特征，在寻找split point的时候，不会对该特征为missing的样本进行遍历统计，只对该列特征值为non-missing的样本上对应的特征值进行遍历，通过这个工程trick来减少了为稀疏离散特征寻找split point的时间开销。在逻辑实现上，为了保证完备性，会分别处理将missing该特征值的样本分配到左叶子结点和右叶子结点的两种情形。


其中$\Omega(f_t)$为每个子树的复杂度。
$$\Omega(f_t) = rT + \frac{1}{2}\lambda||w||^2$$
$T$叶子节点的个数, $||w||$每个叶子节点分数的l2 norm

### 4.2 XGBoost和传统GBDT的区别 

XGBoost对GBDT进行了一系列优化，比如损失函数进行了二阶泰勒展开、目标函数加入正则项、支持并行和默认缺失值处理等，在可扩展性和训练速度上有了巨大的提升

1. **基分类器:** 传统GBDT以CART作为基分类器，xgboost还支持线性分类器，这个时候xgboost相当于带L1和L2正则化项的逻辑斯蒂回归（分类问题）或者线性回归（回归问题);
2. **导数信息:** xgboost使用泰勒公式对损失函数进行二阶的展开，同时用到一阶导数和二阶导数的信息，加速训练过程;
3. **正则项:**xgboost加入正则项，正则项里包含了树的叶子节点个数、每个叶子节点上输出的score的L2模的平方和，用于控制模型的复杂度，防止过拟合。
4. **学习率:** Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）
5. **列抽样（column subsampling):** xgboost借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。
6. **缺失值处理**: 在普通的GBDT策略中，对于缺失值的方法是先手动对缺失值进行填充，然后当做有值的特征进行处理，但是这样人工填充不一定准确，而且没有什么理论依据。而XGBoost采取的策略是先不处理那些值缺失的样本，采用那些有值的样本搞出分裂点，在遍历每个有值特征的时候，尝试将缺失样本划入左子树和右子树，选择使损失最优的值作为分裂点。
7. **并行:** XGBoost的并行，并不是说每棵树可以并行训练，XGBoost本质上仍然采用boosting思想，每棵树训练前需要等前面的树训练完成才能开始训练。XGBoost的并行，指的是特征维度的并行：在训练之前，每个特征按特征值对样本进行预排序，并存储为Block结构，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个block并行计算。

### 4.3 xgboost的常见问题

#### 4.3.1 如何防止过拟合
- 目标函数添加正则项：叶子节点个数+叶子节点权重的L2正则化
- 列抽样：训练的时候只用一部分特征（不考虑剩余的block块即可）
- 子采样：每轮计算可以不使用全部样本，使算法更加保守
- shrinkage: 可以叫学习率或步长，为了给后面的训练留出更多的学习空间
- leaf: 叶子节点个数
- early_stopping: 提取结束
- depth: 数的深度

#### 4.3.2 xgboost为什么要进行二次的展开
- 精确性: 相对于GBDT的一阶泰勒展开，XGBoost采用二阶泰勒展开，可以更为精准的逼近真实的损失函数
- 可扩展性：损失函数支持自定义，只需要新的损失函数二阶可导

#### 4.3.3 xgboost为什么可以并行训练

- XGBoost的并行，并不是说每棵树可以并行训练，XGB本质上仍然采用boosting思想，每棵树训练前需要等前面的树训练完成才能开始训练。

- XGBoost的并行，指的是特征维度的并行：在训练之前，每个特征按特征值对样本进行预排序，并存储为Block结构，在后面查找特征分割点时可以重复使用，而且特征已经被存储为一个个block结构，那么在寻找每个特征的最佳分割点时，可以利用多线程对每个block并行计算。

#### 4.3.4 xgboost为什么快

- 分块并行：训练前每个特征按特征值进行排序并存储为Block结构，后面查找特征分割点时重复使用，并且支持并行查找每个特征的分割点
- 候选分位点：每个特征采用常数个分位点作为候选分割点
- CPU cache 命中优化： 使用缓存预取的方法，对每个线程分配一个连续的buffer，读取每个block中样本的梯度信息并存入连续的Buffer中。
- Block 处理优化：Block预先放入内存；Block按列进行解压缩；将Block划分到不同硬盘来提高吞吐

#### 4.3.5 xgboost如何处理缺失值

- 在特征k上寻找最佳 split point 时，不会对该列特征 missing 的样本进行遍历，而只对该列特征值为 non-missing 的样本上对应的特征值进行遍历，通过这个技巧来减少了为稀疏离散特征寻找 split point 的时间开销。
- 在逻辑实现上，为了保证完备性，会将该特征值missing的样本分别分配到左叶子结点和右叶子结点，两种情形都计算一遍后，选择分裂后增益最大的那个方向（左分支或是右分支），作为预测时特征值缺失样本的默认分支方向。
- 如果在训练中没有缺失值而在预测中出现缺失，那么会自动将缺失值的划分方向放到右子结点。

#### 4.3.6 RF和GBDT的区别
- 集成学习：RF属于bagging思想，而GBDT是boosting思想
- 偏差-方差权衡：RF不断的降低模型的方差，而GBDT不断的降低模型的偏差
- 训练样本：RF每次迭代的样本是从全部训练集中有放回抽样形成的，而GBDT每次使用全部样本
- 并行性：RF的树可以并行生成，而GBDT只能顺序生成(需要等上一棵树完全生成)
- 最终结果：RF最终是多棵树进行多数表决（回归问题是取平均），而GBDT是加权融合
- 数据敏感性：RF对异常值不敏感，而GBDT对异常值比较敏感
- 泛化能力：RF不易过拟合，而GBDT容易过拟合

#### 4.3.7 RF和GBDT的区别
- 调整正负样本的权重 `if (info.labels[i] == 1.0f)  w *= param_.scale_pos_weight`

#### 4.3.8 比较LR和GBDT，说说什么情景下GBDT不如LR
- LR是线性模型，可解释性强，很容易并行化，但学习能力有限，需要大量的人工特征工程
- GBDT是非线性模型，具有天然的特征组合优势，特征表达能力强，但是树与树之间无法并行训练，而且树模型很容易过拟合；
  
**高维稀疏特征的场景下，LR的效果一般会比GBDT好,带正则化的线性模型比较不容易对稀疏特征过拟合。**

#### 4.3.9 数的复杂度如何衡量
通过叶子节点和分数w的二范数

#### 4.3.10 xgboost 如何对树进行剪枝

- 在目标函数中增加了正则项：使用叶子结点的数目和叶子结点权重的L2模的平方，控制树的复杂度。
- 在结点分裂时，定义了一个阈值，如果分裂后目标函数的增益小于该阈值，则不分裂。
- 当引入一次分裂后，重新计算新生成的左、右两个叶子结点的样本权重和。如果任一个叶子结点的样本权重低于某一个阈值（最小样本权重和），也会放弃此次分裂。
- XGBoost 先从顶到底建立树直到最大深度，再从底到顶反向检查是否有不满足分裂条件的结点，进行剪枝。

#### 4.3.11 如何选择最优的分裂点

- XGBoost在训练前预先将特征按照特征值进行了排序，并存储为block结构，以后在结点分裂时可以重复使用该结构。
- 因此，可以采用特征并行的方法利用多个线程分别计算每个特征的最佳分割点，根据每次分裂后产生的增益，最终选择增益最大的那个特征的特征值作为最佳分裂点。
- 如果在计算每个特征的最佳分割点时，对每个样本都进行遍历，计算复杂度会很大，这种全局扫描的方法并不适用大数据的场景。XGBoost还提供了一种直方图近似算法，对特征排序后仅选择常数个候选分裂位置作为候选分裂点，极大提升了结点分裂时的计算效率。

#### 4.3.12 如何选择最优的分裂点

- weight ：该特征在所有树中被用作分割样本的特征的总次数。
- gain ：该特征在其出现过的所有树中产生的平均增益。
- cover ：该特征在其出现过的所有树中的平均覆盖范围

#### 4.3.13 如何调参

- learning rate
- estimator
- max_depth, num_layers
- min_child_weight: 如果一个结点分裂后，它的所有子节点的权重之和都大于该阈值，该叶子节点才可以划分
- gamma: 对于一个叶子节点，当对它采取划分之后，损失函数的降低值的阈值。
- subsample, colsample_bytree
- 正则化参数

#### 4.3.14 为什么XGBoost相比某些模型对缺失值不敏感

对存在缺失值的特征，一般的解决方法是：

- 离散型变量：用出现次数最多的特征值填充；
- 连续型变量：用中位数或均值填充；

一些模型如SVM和KNN，其模型原理中涉及到了对样本距离的度量，如果缺失值处理不当，最终会导致模型预测效果很差。
而树模型对缺失值的敏感度低，大部分时候可以在数据缺失时时使用。原因就是，一棵树中每个结点在分裂时，寻找的是某个特征的最佳分裂点（特征值），完全可以不考虑存在特征值缺失的样本，也就是说，如果某些样本缺失的特征值缺失，对寻找最佳分割点的影响不是很大。

## 5. LightGBM


### 5.1 lightgbm 和xgboost的区别

- 树的生长策略: XGB采用level-wise的分裂策略，LGB采用leaf-wise的分裂策略
- 分割点查找算法：XGB使用特征预排序算法，LGB使用基于直方图的切分点算法
- 支持离散变量：xgboost无法直接输入类别型变量，因此需要事先对类别型变量进行编码（例如独热编码），而LightGBM可以直接处理类别型变量。
- 并行策略


## 参考资料

- [机器学习算法中 GBDT 和 XGBOOST 的区别有哪些？](https://www.zhihu.com/question/41354392)
- [珍藏版 | 20道XGBoost面试题，你会几个？(下篇)](https://mp.weixin.qq.com/s/BbelOsYgsiOvwfwYs5QfpQ)