# 信息论的基本概念

信息论是研究信号保护多少信息量的量化，即关于信息的量化的科学。本文总结常见的信息论的基本概念。

## 信息量
每个时间的发生都有一定的信息量，如"明天太阳从东边升起"这句话的信息量就很少，因为这是必然发生的时间，因此信息量与一个时间发生的概率是相关的，定义一个事件 $X=x$的信息量为：

$$I(X=x) = -logp(x)$$
因此可以看到，当事件x概率越小时，其所包含的信息量就越大，当其为必然事件时，其信息量为0，符合定义。

## 熵

熵时对整个分布中不确定性的总量进行量化，其实信息量的期望值:
$$H(X) = E(I(x)) = \sum_i^np(x_i).logp(x_i)$$

## K-L散度
k-l散度又称为相对熵(relative entropy)是衡量一个随机变量X的两个概率分布p(x)和q(x)的差异：

$$D_{kl}(p||q) = E_{x-p}[log(\frac{p(x)}{q(x)}] = \sum_i^np(x_i).log(\frac{p(x_i)}{q(x_i)})$$

p(x)往往代表真实分布，q代表模型预测分布，因此当q接近于p时，散度值就越小

## 交叉熵

交叉熵(cross entropy)是一种衡量估计模型和真实概率分布直接差异的情况，如果一个随机变量X服从X~P(x),q(x)用于近似q(x)的概率分布，那么随机变量X和模型q的交叉熵为：

$$H(X,q) = H(X) + D(p||q) = -\sum_i^np(x_i).logq(x_i)$$


具体的将K-L散度进行展开：

$$D_{kl}(p||q) = \sum_i^np(x_i).logp(x_i) -\sum_i^np(x_i).logq(x_i) = -H(p) - H(p,q)$$

因为H(P)为真实的分布，一般不会改变，所以，只需使用交叉熵作为损失函数即可用于训练的过程了。

## 二分类的交叉熵损失

一般面试会问到交叉熵损失函数，对于二分类的问题而言，交叉熵可以写成：

$$H(p,q) = -[y_ilog(q) + (1-y_i).log(1-q)]$$
这也就是二分类交叉熵的来源了。