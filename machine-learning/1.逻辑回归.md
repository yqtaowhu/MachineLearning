## 1. 逻辑回归基本原理
### 1.1 逻辑回归为什么使用log损失

逻辑回归的基本假设为:服从伯努利分布

训练集 $D = {(x^1,y^1),...,(x^m,y^m)}$,其中$x^i=(x_1^i,x_2^i,...x_n^i)为第i个样本的n个特征$

$$
\begin{aligned}
    h_\theta(x) &= g(\theta^T.x) = \frac{1}{1+e^{-\theta^Tx}} \\
   L(\theta) &= \prod_i^mP(y=1|x)^{y_i}.P(y=0)P(y=0|x)^{1-y_i}\\
   log(L(\theta)) &= \sum_i^m[y_i\log{P(y=1|x)} + (1-y_i)\log{P(y=0|x)}] \\
所以整体样板的损失函数为: \\
   cost(h_\theta(x_i), y_i)&=\sum_i^m[-y_i.logh_\theta(x_i) + (1-y_i).log(1-h_\theta(x_i))]\\
   
\end{aligned}
$$


### 1.2 推导优化公式
$$
\begin{aligned}
对于单个样板:\\
J(\theta)&=-y.logh_\theta(x) - (1-y).log(1-h_\theta(x))\\

\frac{\partial(J)}{\partial\theta_j}
&=[y.\frac{1}{g(\theta^T.x)}-(1-y).\frac{-1}{1-g(\theta^T.x)}]\frac{\partial{g(\theta^T.x)}}{\partial{\theta_j}}\\
&=-[y.\frac{1}{g(\theta^T.x)}-(1-y).\frac{1}{1-g(\theta^T.x)}].g(\theta^Tx)(1-g(\theta^T.x)).x_j \\
&=-[y(1-g(\theta^T.x))-(1-y).g(\theta^T.x)]x_j \\
&=(h_\theta(x)-y).x_j \\

\therefore\\
\theta_j &:= \theta_j - \alpha((h_\theta(x)-y).x_j)\\
\therefore 对于批量样本 \\
\theta_j &:= \theta_j - \sum_i^m\alpha((h_\theta(x^i)-y^i).x_j^i)

\end{aligned}
$$

### 1.3 如何进行向量化
$$
\begin{aligned}
    \vec{x} &= [x^1,x^2,...,x^m]: x_n^m表示第m个样本的第n个特征\\
    \vec{y}&=[y^1,y^2,...y^m]\\
    误差E:\\
    \vec{E} &= h_\theta(x) - y : m*1向量\\
    考虑单个特征:\\
    \theta_0 &= \theta_0 - \alpha\sum_i^m[h_\theta(x^i).-y^i].x_j^i \\ 
    &= \theta_0 - \alpha{E}*(x_0^1,x_0^2,...x_0^m)\\
    \therefore\\
    \theta &= \theta - \alpha{X^T}E

\end{aligned}
$$

## 2. 逻辑斯特回归为什么要对特征进行离散化
- 逻辑回归是一种广义的线性模型，表达能力有限，单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散特征的增加和减少都很容易，易于模型的快速迭代；
- 速度快！速度快！速度快！稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展
-  鲁棒性！鲁棒性！鲁棒性！离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
-  方便交叉与特征组合：离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
-   稳定性：特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；
-   简化模型：特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。

李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。


## 参考资料
- [逻辑斯特回归为什么要对特征进行离散化？](https://zhuanlan.zhihu.com/p/61049356)