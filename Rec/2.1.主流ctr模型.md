- [概述](#概述)
  - [1. LR(logistic regression)](#1-lrlogistic-regression)
  - [2. GBDT + LR](#2-gbdt--lr)
  - [3. FM, FFM](#3-fm-ffm)
  - [4. Wide \& Deep](#4-wide--deep)
  - [5. FNN(Factorization-machine supported Neural Network)](#5-fnnfactorization-machine-supported-neural-network)
  - [6. PNN（Product-based Neural Networks）](#6-pnnproduct-based-neural-networks)
  - [7.DeepFM](#7deepfm)
  - [8. Deep \& Cross](#8-deep--cross)
  - [9. DIN](#9-din)
- [参考资料](#参考资料)

#  概述
&emsp;CTR预估(Click-through rate prediction)是工业界中重要的应用，如在线广告，本文主要介绍目前主流的ctr模型从传统的**浅层模型**到目前的**深度模型**进行简单的介绍。
&emsp;在ctr的应用场景中，数据的特点一般是：高维、稀疏、多域的，一般对不同的域进行one-hot或multi-hot操作操作，如下图所示，一个电商中的场景中，weekday进行one-hot的表示，和visited_cat_ids的multi-hot的表示:

![data](resource/data.png)
通常对于one-hot特征一般进行embedding-based的方法将one-hot映射称**低维稠密向量**，用来减少数据维度。


## 1. LR(logistic regression)
lr是一种广义的线性模型，可以看做是一个没有隐层的神经网络模型,其公式为:
$$y = \frac{1}{1+e^{-w^Tx}}$$

LR模型一直是CTR预估问题的benchmark模型，由于其简单、易于并行化实现、可解释性强等优点而被广泛使用。然而由于线性模型本身的局限，不能处理特征和目标之间的非线性关系，因此模型效果严重依赖于算法工程师的特征工程经验。

**一个自然的问题是如何学习原始特征和拟合目标之间的非线性关系？**

1. 连续特征离散化
   如将年龄这种连续的特征进行离散化，分段处理，增加模型的鲁棒性
2. 特征交叉
   非常重要的特征工程，单个特征的贡献弱，组合不同的特征可能具有较强的相关性。如将”男性“和”足球“进行特征组合，特征交叉简单，可解释性强，易并行，但是需要大量的特征工程。

## 2. GBDT + LR 
特征工程很难做，且比较耗时，那么如何进行自动完成呢？一种解决思路是使用模型级联，我们知道树模型可以做特征的非线性变化。因此将特征分成两个部分
1. GBDT: 对连续值、值空间不大的category特征
2. LR: 空间很大的ID特征
一般使用GBDT输出的叶子节点进行编号，然后使用LR进行训练。
两者的组合既能做高阶的特征组合(GBDT),又能利用线性模型易于处理大规模数据的优势(LR)，将GBDT和LR的优势整合起来。

## 3. FM, FFM
LR模型中很容易引入特征组合，如一个简单的二阶特征组合方式:
$$\hat{y} = w_0 + \sum_{i=1}^nw_i.x + \sum_{i=1}^n\sum_{j=i+1}^nw_{i,j}.x_ix_j$$
其优点是加入了特征组合，但是缺点是，组合特征的泛华能力弱。因为只有当$x_i, x_j$同时出现时，$w_{i,j}$才不等于0，否自其为0.尤其是在大规模稀疏特征存在的场景下，比如CTR预估和推荐排序，这些场景的最大特点就是特征的大规模稀疏

因子分解机(Factorization Machines, FM)通过特征对之间的隐变量内积来提取特征组合，其函数形式如下：
$$\hat{y} = w_0 + \sum_{i=1}^nw_i.x + \sum_{i=1}^n\sum_{j=i+1}^n<v_i,v_j>x_ix_j   （1）$$
$$<v_i,v_j> = \sum_f^k{v_{i,f}{v_{j,f}}}, 即两个k维向量的乘积: v_i^T.v_j$$

FM对于每个特征，学习一个大小为k的一维向量，于是，两个特征 $v_i$ 和$v_j$ 的特征组合的权重值，这本质上是在对特征进行embedding化表征，和目前非常常见的各种实体embedding本质思想是一脉相承的，

在实现中，通常会随机的初始化每个特征为k维的向量，然后通过训练，最终学习到每个特征的embedding向量。

**可以看到，即使$x_i,x_j$没有同时出现，其权重值仍然不为0，因此泛化能力得到提升**

上述公式1可以看到，其实o(n^2)的，在大规模的稀疏数据场景中是不可用的，但是其可以通过公式变成o(kn)的常数复杂度:

![fm.png](resource/fm.png)

FM中对于每一个特征的embedding是一样的，但是在FFM中，会有$F-1$个embdding向量，表明了当一个特征和不同的域进行特征组合是使用不同的embedding向量。
$$\hat{y} = w_0 + \sum_{i=1}^nw_i.x + \sum_{i=1}^n\sum_{j=i+1}^n<v_{i,f_j},v_{j,f_i}>x_ix_j  $$

## 4. Wide & Deep
wide & deep 核心思路:

wide : memorization
deep : generalization
wide 部分需要人工进行特征工程

![wide & deep](https://pic2.zhimg.com/v2-a317a9fb4bbc943a5bc894924dc997f5_r.jpg)

wide部分与LR模型并没有什么区别；deep部分则是先对不同的ID类型特征做embedding，在embedding层接一个全连接的MLP（多层感知机），用于学习特征之间的高阶交叉组合关系。由于Embedding机制的引入，WDL相对于单纯的wide模型有更强的泛化能力。

## 5. FNN(Factorization-machine supported Neural Network)

1. fm学习到embedding向量
2. 对embedding向量进行nn操作

## 6. PNN（Product-based Neural Networks）

PNN主要的想法: **使用product操作来捕捉不同field特征**

PNN主要是在深度学习网络中增加了一个inner/outer product layer，用来建模特征之间的关系。其在embedding层后添加了product层。

![pnn](https://pic1.zhimg.com/v2-4d8262eddb0e37f8efcf031b48c79e80_r.jpg)


## 7.DeepFM
- FNN， 将fm的embedding向量应用于dnn
- PNN， 在embedding层添加product层
- wide & deep ， 需要手工进行提取特征
fnn,pnn两者都难以提取低价的特征,wide & deep需要大量的特征工程。因此想到使用fm自动提取低阶特征。

![deepfm](https://pic3.zhimg.com/v2-fdfa81ae81648fc44eda422bfbcee572_r.jpg)

**DeepFM共享相同的输入与embedding向量**

$$\hat{y} = sigmoid(y_{FM} + y_{DNN})$$
$$y_{FM} = w0 + \sum_{i=1}\sum_{j=i+1}<vi,vj>x_ix_j$$
$$y_{DNN} = sigmoid(w^{(l+1)}.a^l + b^{(l+1)})$$

## 8. Deep & Cross
- wide & depp: 人工的特征工程
- FM : 二阶的交叉
很自然的问题，如何自动学习高阶的特征组合呢？

![deep&cross](https://pic1.zhimg.com/v2-5dc2141a3fb667a0cdf13aa4c1c65e38_r.jpg)


如上图所示，模型的输入层,将稀疏特征进行embedding, 然后和dense特征进行组合。

$$x_0 = [x_{embed,1}^T,x_{embed,2}^T,...x_{embed,k}^T,x_{dense}^T]$$

然后分成两个网络，分别是corss network 和 deep network.

**交叉网络**

$$x_{l+1} = x_0.x_l^T.w_l + b_l + x_l$$
随着层数l的增加，其特征的阶数也在增加。

如下图所示的计算过程：

![x_l](https://pic4.zhimg.com/v2-3847dc228451b9f35a8305b1e34bff67_r.jpg)


## 9. DIN

1. **Embedding & MLP paradigm**

    In these methods large scale sparse input
features are first mapped into low dimensional embedding vectors,and then transformed into fixed-length vectors in a group-wise
manner, finally concatenated together to fed into a multilayer perceptron (MLP) to learn the nonlinear relations among features.

    这种对用户特征进行特定长度向量表达的方式，难以捕捉用户的多种兴趣。

2. **用户行为数据的两个观测**
   - diverity: 多样性，一个用户可能对多种品类感兴趣。
   - local activation: 部分激活，只有一部分历史数据对目前的点击率有帮助。

3. 如何表达用户兴趣 user reprentation

$$V_u = f(V_a) = \sum_i^Nw_i.V_i = \sum_i^Ng(V_i,V_a).V_i$$

其中：

- $V_u$: 用户的embedding向量
- $V_a$: 是候选广告商品的embedding向量
- $V_i$: 是用户u的第i次行为的embedding向量，因为这里用户的行为就是浏览商品或店铺，所以行为的embedding的向量就是那次浏览的商品或店铺的embedding向量。
- $V_i$的权重由$V_i$和$V_a$决定。

**因为加入了注意力机制，$V_u$从过去 $V_i$的加和变成了$V_i$ 的加权和。**从下图可以看到，权重的计算方式是使用用户和广告，和两者的out product然后进行堆叠，进行mlp，计算得来的。

![din](https://pic2.zhimg.com/v2-1ac40e970e1f76c16e64474d359d0829_r.jpg)

4. mini-batch aware regularizer
   
5. activation function  : 
用Dice方法替代经典的PReLU激活函数


# 参考资料
- [推荐系统召回四模型之：全能的FM模型](https://zhuanlan.zhihu.com/p/58160982)
- [主流CTR预估模型的演化及对比](https://zhuanlan.zhihu.com/p/35465875)
- [玩转企业级Deep&Cross Network模型你只差一步](https://zhuanlan.zhihu.com/p/43364598)
- [推荐系统中的注意力机制——阿里深度兴趣网络（DIN）](https://zhuanlan.zhihu.com/p/51623339)
- [阿里DIN网络你真的懂了吗？](https://zhuanlan.zhihu.com/p/81193062)
- Deep Interest Network for Click-Through Rate Prediction



