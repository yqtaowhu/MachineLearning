- [词向量总结](#词向量总结)
  - [skip-gram和cbow的区别](#skip-gram和cbow的区别)
  - [Transformers](#transformers)
  - [BERT／GPT／Elmo的区别？BERT的优点在哪？](#bertgptelmo的区别bert的优点在哪)
  - [参考资料](#参考资料)


# 词向量总结

## skip-gram和cbow的区别
1. CBOW 

在cbow方法中，是用周围词预测中心词，从而利用中心词的预测结果情况，使用GradientDesent方法，不断的去调整周围词的向量。当训练完成之后，每个词都会作为中心词，把周围词的词向量进行了调整，这样也就获得了整个文本里面所有词的词向量。
可以看到，cbow预测行为的次数跟整个文本的词数几乎是相等的（每次预测行为才会进行一次backpropgation, 而往往这也是最耗时的部分），复杂度大概是O(V);

2. Skip-gram

而skip-gram是用中心词来预测周围的词。在skip-gram中，会利用周围的词的预测结果情况，使用GradientDecent来不断的调整中心词的词向量，最终所有的文本遍历完毕之后，也就得到了文本所有词的词向量。

可以看出，skip-gram进行预测的次数是要多于cbow的：因为每个词在作为中心词时，都要使用周围词进行预测一次。这样相当于比cbow的方法多进行了K次（假设K为窗口大小），因此时间的复杂度为O(KV)，训练时间要比cbow要长。

但是在skip-gram当中，每个词都要收到周围的词的影响，每个词在作为中心词的时候，都要进行K次的预测、调整。因此， 当数据量较少，或者词为生僻词出现次数较少时， 这种多次的调整会使得词向量相对的更加准确

3. 总结

skim-gram要好于cbow, 因为一个中心词，用很多周边的词取训练他，但训练速度

## Transformers
seq2seq model with self-attention


## BERT／GPT／Elmo的区别？BERT的优点在哪？

1. elmo 

- ELMo主要是用来做特征抽取，应用到下游任务的时候，模型参数不更新
- 架构上是利用两个双向LSTM，一个对输入句子进行正向编码，一个对输入句子进行反向编码，然后将两个编码连接在一起，作为下游任务的输入。

2. GPT
 
- GPT可以用来微调，应用到下游任务的时候，可以根据具体任务，更新模型参数
- 架构上是堆叠的单向Transformer，可以看作是Encoder-Decoder架构的Encoder部分，但是预训练的时候为了避免self-attention看到整句的信息，借鉴了Decoder的Mask方法，就是训练到时刻t的时候，将t+1以及之后的信息屏蔽掉，这也就是单向的原因。

3. bert: Bidirectional Encoder Representation from Transformers
- 双向的transfermer作为特征抽取器(双向语言模型)
- Bert最关键两点，一点是特征抽取器采用Transformer；第二点是预训练的时候采用双向语言模型。



ELMo虽然是双向的，但是无法将上下文编码在同一个向量里，比如

“这个小狗无法跳上台阶因为它太累了”或者“这个小狗无法跳上台阶因为它太高了”

正向编码无法得知“它”是谁，因为看不到后边的“累”或者“高”

反向编码无法得知“它”是谁，因为看不到前边的“小狗”或者“台阶”

GPT中的Self-Attention本身是可以同时利用“它”前边和后边的信息，但是为了防止看到未来的信息，引入了Mask机制，所以其实看不到后边的信息。

GPT采用的是普通的语言模型的策略，也就是根据历史信息预测当前信息。BERT为了解决GPT中看不到未来信息，只能利用一个方向的信息的问题，引入了Masked LM。类似完形填空，训练的时候，遮蔽一些词，然后让BERT预测这些词，目标函数就是使得预测的词尽可能正确。

强迫模型在编码当前时刻的时候不能太依赖于当前的词，而要考虑它的上下文，甚至更加上下文进行”纠错”。

OpenAI GPT使用的是BooksCorpus语料，总的词数800M；而BERT还增加了wiki语料，其词数是2,500M。所以BERT训练数据的总词数是3,300M。


## 参考资料

- [从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史](https://zhuanlan.zhihu.com/p/49271699)