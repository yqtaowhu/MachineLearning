- [FM\&FFM\&](#fmffm)
  - [FM 相比于LR的优势](#fm-相比于lr的优势)
  - [FM的缺点](#fm的缺点)
  - [FFM（Field-aware Factorization Machines，场感知因子分解机)](#ffmfield-aware-factorization-machines场感知因子分解机)
  - [实现FM \& FFM](#实现fm--ffm)
  - [wide \& deep优化器](#wide--deep优化器)
    - [1. **模型结构差异导致的不同优化需求**](#1-模型结构差异导致的不同优化需求)
    - [2. **特征处理的本质区别**](#2-特征处理的本质区别)
    - [3. **实际工程经验**](#3-实际工程经验)
    - [4. **理论支持**](#4-理论支持)
    - [对比总结表](#对比总结表)

# FM&FFM&

## FM 相比于LR的优势

总结要回答出: 二阶交叉，稀疏受训问题，隐向量表达。

1. **特征组合建模能力**：  
   FM可以自动学习特征之间的二阶交互关系，而LR需要手动设计和引入特征交叉。这使得FM在处理高维稀疏数据时具有更强的表达能力。

2. **解决数据稀疏问题**：  
   在稀疏数据场景下，LR难以有效估计特征组合的权重，而FM通过将特征映射到低维隐向量空间，并利用向量内积来建模特征之间的相关性，从而缓解了这一问题。

3. **泛化能力增强**：  
   由于FM引入了隐向量机制，即使某些特征组合在训练集中未出现，也能通过已知特征的隐向量进行合理预测，提升了模型的泛化性能。

4. **线性时间复杂度计算优化**：  
   FM的二阶项可以通过数学变换实现线性时间复杂度的高效计算（即`O(kn)`），避免了直接计算所有特征组合带来的计算爆炸问题。

5. **通用性强**：  
   FM是一种通用的特征建模方法，可以适用于各种任务（如分类、回归、推荐等），并且可以直接建模one-hot编码后的类别型特征，非常适合推荐系统等实际应用场景。

## FM的缺点

1. 仅建模二阶特征交互：
FM默认只对特征之间的二阶交叉关系进行建模，无法直接捕捉更高阶的特征组合效应，这在某些复杂任务中可能限制其表达能力。

2. 隐向量维度敏感：
隐向量的维度 k 是一个超参数，对模型性能影响较大。若设置不合理，可能导致欠拟合或过拟合，需要通过调参来优化。

3. 线性部分依赖手工特征：
虽然FM能自动学习二阶交叉特征，但其线性部分仍依赖于原始特征的输入形式，可能需要一定的特征工程来提升效果。

4. 不同filed的向量固定，对所有的特征都是一样的


## FFM（Field-aware Factorization Machines，场感知因子分解机)
增加filed的概念，每个特征每个filed一个。


## 实现FM & FFM

```

import torch
import torch.nn as nn

class FM(nn.Module):
    def __init__(self, num_features, embedding_dim):
        super(FM, self).__init__()
        self.linear = nn.Linear(num_features, 1)  # 线性部分
        self.embedding = nn.Embedding(num_features, embedding_dim)  # 特征嵌入
        
    def forward(self, y, x):
        # x shape: (batch_size, num_features)
        linear_term = self.linear(y)
        
        # 二阶交叉项计算
        embedded = self.embedding(x)
        sum_square = torch.sum(embedded, dim=1).pow(2)
        square_sum = torch.sum(embedded.pow(2), dim=1)
        interaction_term = 0.5 * (sum_square - square_sum).sum(dim=1, keepdim=True)
        
        return torch.sigmoid(linear_term + interaction_term)


class FFM(nn.Module):
    def __init__(self, num_features, num_fields, embedding_dim):
        super(FFM, self).__init__()
        self.linear = nn.Linear(num_features, 1)  # 线性部分
        self.num_fields = num_fields
        self.embedding = nn.Embedding(num_features * num_fields, embedding_dim)
        
    def forward(self, x, field_map):
        # x shape: (batch_size, num_features)
        # field_map: 各特征所属的field ID列表
        linear_term = self.linear(x)
        
        # 二阶交叉项计算
        interaction_term = 0
        for i in range(x.size(1)):
            for j in range(i+1, x.size(1)):
                # 获取特征i在特征j的field下的嵌入
                vi = self.embedding(torch.tensor(i * self.num_fields + field_map[j]))
                # 获取特征j在特征i的field下的嵌入
                vj = self.embedding(torch.tensor(j * self.num_fields + field_map[i]))
                interaction_term += torch.sum(vi * vj) * x[:, i] * x[:, j]
        
        return torch.sigmoid(linear_term + interaction_term.unsqueeze(1))
```

## wide & deep优化器
- FTRL
- Adagrad

在Wide & Deep模型中，Wide部分和Deep部分分别使用不同的优化器（如论文原文所述，Wide部分使用FTRL，Deep部分使用AdaGrad），这种设计主要基于以下几个方面的考虑：

---

### 1. **模型结构差异导致的不同优化需求**
   - **Wide部分（线性模型）**：
     - 采用**FTRL（Follow-the-Regularized-Leader）**优化器：
       - 适合处理高维稀疏特征（如one-hot编码的类别特征）
       - 天然支持L1正则化，可以自动做特征选择，减少模型复杂度
       - 在线学习场景下表现优秀，适合实时更新模型参数
   - **Deep部分（神经网络）**：
     - 采用**AdaGrad**优化器：
       - 自适应调整学习率，适合深度网络的层级梯度更新
       - 对稀疏特征嵌入（Embedding）的优化更稳定
       - 能缓解神经网络训练中的梯度消失/爆炸问题

---

### 2. **特征处理的本质区别**
   - **Wide侧特征**：
     - 多为人工设计的交叉特征（如`user_id × item_category`）
     - 需要快速收敛和稀疏性保持（FTRL的特点）
   - **Deep侧特征**：
     - 通过Embedding层学习稠密向量表示
     - 需要稳定训练和自适应学习率（AdaGrad的优势）

---

### 3. **实际工程经验**
   - **内存和计算效率**：
     - FTRL对Wide部分的稀疏参数更新更高效
     - AdaGrad对Deep部分的稠密参数优化更节省内存
   - **解耦训练过程**：
     - 两部分可以独立调整超参数（如学习率、正则化强度）
     - 避免单一优化器难以同时兼顾线性模型和深度网络的矛盾需求

---

### 4. **理论支持**
   - FTRL的**稀疏性诱导特性**与Wide部分的特征工程目标一致（论文第4节强调）
   - AdaGrad的**累积梯度平方归一化**更适合深度网络的非凸优化问题

---

### 对比总结表
| 优化器特性       | Wide部分（FTRL）                | Deep部分（AdaGrad）             |
|------------------|---------------------------------|---------------------------------|
| **适用模型**     | 线性模型                        | 深度神经网络                    |
| **正则化支持**   | L1正则化（自动特征选择）        | 需额外添加L2正则化              |
| **稀疏性处理**   | 优秀（适合one-hot特征）         | 一般（适合Embedding特征）       |
| **学习率调整**   | 固定学习率                      | 自适应调整                      |
| **在线学习**     | 支持                            | 不支持                          |

---