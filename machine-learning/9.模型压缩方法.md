
- [模型压缩方法综述](#模型压缩方法综述)
  - [网络剪枝(network pruning)](#网络剪枝network-pruning)
  - [知识蒸馏(knowledge distillation)](#知识蒸馏knowledge-distillation)
  - [参数量化(parameter quantization)](#参数量化parameter-quantization)
  - [架构设计(architecture design)](#架构设计architecture-design)
  - [动态计算(dynamic computation)](#动态计算dynamic-computation)
  - [参考资料](#参考资料)

# 模型压缩方法综述

## 网络剪枝(network pruning)

1. 训练一个比较大的网络
2. 评估weight或者neuron的重要性
3. 删除重要性低的权重或者神经元
4. fine-turning
5. 符合预期结束，否则重复2

**为什么不开始就用小的网络去训练**
因为小的网络难以训练，而大的网络容易进行优化。


## 知识蒸馏(knowledge distillation)

思想是使用一个大的网络去训练一个模型，然后使用一个小的网络取学习大的网络的结果，比如分类问题，大的网络输出的是一个向量，每个类别都有概率[0.9,0.05, 0.05]，小的网络就学习这个向量，而不是学习真实的标签[1,0,0]。

## 参数量化(parameter quantization)

- 使用低bit代替高bit的(如float32替代float64)
- 二值网络

## 架构设计(architecture design)

- 全连接层 N*M(10, 1000)，可以添加一个更少参数的隐含层，如 N*V*M(10,5,1000)这种方式可以大量进行全连接成的参数
- cnn: 将标准的cnn转换成depth-wise cnn和point-wise cnn的两种操作，可以减少参数量

## 动态计算(dynamic computation)

- 训练多个分类器，在不同的时机使用
- 在比较深层的网络中间添加分类层


## 参考资料

- [深度学习模型压缩与加速综述
SIGAI](https://zhuanlan.zhihu.com/p/67871864)
- 机器学习(李宏毅)